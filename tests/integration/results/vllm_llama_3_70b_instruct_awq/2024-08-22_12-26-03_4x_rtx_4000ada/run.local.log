2024-08-22 12:26:03,711 - __main__ - INFO - Starting experiment vllm_llama_3_70b_instruct_awq with comment: 4x RTX 4000Ada
2024-08-22 12:26:03,714 - __main__ - INFO - Local log file: /home/rooter/dev/bac/deterministic-ml/tests/integration/results/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/run.local.log
2024-08-22 12:26:03,882 - paramiko.transport - INFO - Connected (version 2.0, client OpenSSH_8.9p1)
2024-08-22 12:26:04,053 - paramiko.transport - INFO - Auth banner: b'Welcome to vast.ai. If authentication fails, try again after a few seconds, and double check your ssh key.\nHave fun!\n'
2024-08-22 12:26:04,057 - paramiko.transport - INFO - Authentication (publickey) successful!
2024-08-22 12:26:04,062 - __main__ - INFO - Syncing files to remote
2024-08-22 12:26:04,290 - tools.ssh - INFO - Command: 'mkdir -p ~/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/output' stdout: '' stderr: '' status_code: 0
2024-08-22 12:26:06,997 - __main__ - INFO - Setting up remote environment
2024-08-22 12:26:45,244 - tools.ssh - INFO - Command: '\n    set -exo pipefail\n    \n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=$HOME/.cargo/bin:$PATH\n    \n    cd ~/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada\n    uv venv -p python3.11 --python-preference managed\n    source .venv/bin/activate \n    uv pip install       ./deterministic_ml*.whl       pyyaml       -r vllm_llama_3_70b_instruct_awq/requirements.txt\n    ' stdout: "installing to /root/.cargo/bin\n  uv\n  uvx\neverything's installed!\n\nTo add $HOME/.cargo/bin to your PATH, either restart your shell or run:\n\n    source $HOME/.cargo/env (sh, bash, zsh)\n    source $HOME/.cargo/env.fish (fish)\n" stderr: "+ curl -LsSf https://astral.sh/uv/install.sh\n+ sh\ndownloading uv 0.3.1 x86_64-unknown-linux-gnu\n+ export PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ cd /root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada\n+ uv venv -p python3.11 --python-preference managed\nUsing Python 3.11.9\nCreating virtualenv at: .venv\nActivate with: source .venv/bin/activate\n+ source .venv/bin/activate\n++ '[' -n x ']'\n++ SCRIPT_PATH=.venv/bin/activate\n++ '[' .venv/bin/activate = bash ']'\n++ deactivate nondestructive\n++ unset -f pydoc\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ hash -r\n++ '[' -z '' ']'\n++ unset VIRTUAL_ENV\n++ unset VIRTUAL_ENV_PROMPT\n++ '[' '!' nondestructive = nondestructive ']'\n++ VIRTUAL_ENV=/root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/.venv\n++ '[' linux-gnu = cygwin ']'\n++ '[' linux-gnu = msys ']'\n++ export VIRTUAL_ENV\n++ _OLD_VIRTUAL_PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ PATH=/root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/.venv/bin:/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ '[' x2024-08-22_12-26-03_4x_rtx_4000ada '!=' x ']'\n++ VIRTUAL_ENV_PROMPT=2024-08-22_12-26-03_4x_rtx_4000ada\n++ export VIRTUAL_ENV_PROMPT\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ _OLD_VIRTUAL_PS1=\n++ PS1='(2024-08-22_12-26-03_4x_rtx_4000ada) '\n++ export PS1\n++ alias pydoc\n++ true\n++ hash -r\n+ uv pip install ./deterministic_ml-0.1.dev2+g218f083.d20240822-py3-none-any.whl pyyaml -r vllm_llama_3_70b_instruct_awq/requirements.txt\nResolved 108 packages in 885ms\nPrepared 108 packages in 31.88s\nInstalled 108 packages in 489ms\n + aiohappyeyeballs==2.4.0\n + aiohttp==3.10.5\n + aiosignal==1.3.1\n + annotated-types==0.7.0\n + anyio==4.4.0\n + attrs==24.2.0\n + certifi==2024.7.4\n + charset-normalizer==3.3.2\n + click==8.1.7\n + cloudpickle==3.0.0\n + cmake==3.30.2\n + datasets==2.21.0\n + deterministic-ml==0.1.dev2+g218f083.d20240822 (from file:///root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/deterministic_ml-0.1.dev2+g218f083.d20240822-py3-none-any.whl)\n + dill==0.3.8\n + diskcache==5.6.3\n + distro==1.9.0\n + fastapi==0.112.1\n + filelock==3.15.4\n + frozenlist==1.4.1\n + fsspec==2024.6.1\n + h11==0.14.0\n + httpcore==1.0.5\n + httptools==0.6.1\n + httpx==0.27.0\n + huggingface-hub==0.24.6\n + idna==3.7\n + interegular==0.3.3\n + jinja2==3.1.4\n + jiter==0.5.0\n + jsonschema==4.23.0\n + jsonschema-specifications==2023.12.1\n + lark==1.2.2\n + llvmlite==0.43.0\n + lm-format-enforcer==0.10.3\n + markupsafe==2.1.5\n + mpmath==1.3.0\n + msgpack==1.0.8\n + multidict==6.0.5\n + multiprocess==0.70.16\n + nest-asyncio==1.6.0\n + networkx==3.3\n + ninja==1.11.1.1\n + numba==0.60.0\n + numpy==1.26.4\n + nvidia-cublas-cu12==12.1.3.1\n + nvidia-cuda-cupti-cu12==12.1.105\n + nvidia-cuda-nvrtc-cu12==12.1.105\n + nvidia-cuda-runtime-cu12==12.1.105\n + nvidia-cudnn-cu12==9.1.0.70\n + nvidia-cufft-cu12==11.0.2.54\n + nvidia-curand-cu12==10.3.2.106\n + nvidia-cusolver-cu12==11.4.5.107\n + nvidia-cusparse-cu12==12.1.0.106\n + nvidia-ml-py==12.560.30\n + nvidia-nccl-cu12==2.20.5\n + nvidia-nvjitlink-cu12==12.6.20\n + nvidia-nvtx-cu12==12.1.105\n + openai==1.42.0\n + outlines==0.0.46\n + packaging==24.1\n + pandas==2.2.2\n + pillow==10.4.0\n + prometheus-client==0.20.0\n + prometheus-fastapi-instrumentator==7.0.0\n + protobuf==5.27.3\n + psutil==6.0.0\n + py-cpuinfo==9.0.0\n + pyairports==2.1.1\n + pyarrow==17.0.0\n + pycountry==24.6.1\n + pydantic==2.8.2\n + pydantic-core==2.20.1\n + python-dateutil==2.9.0.post0\n + python-dotenv==1.0.1\n + pytz==2024.1\n + pyyaml==6.0.2\n + pyzmq==26.2.0\n + ray==2.34.0\n + referencing==0.35.1\n + regex==2024.7.24\n + requests==2.32.3\n + rpds-py==0.20.0\n + safetensors==0.4.4\n + sentencepiece==0.2.0\n + setuptools==73.0.1\n + six==1.16.0\n + sniffio==1.3.1\n + starlette==0.38.2\n + sympy==1.13.2\n + tiktoken==0.7.0\n + tokenizers==0.19.1\n + torch==2.4.0\n + torchvision==0.19.0\n + tqdm==4.66.5\n + transformers==4.44.1\n + triton==3.0.0\n + typing-extensions==4.12.2\n + tzdata==2024.1\n + urllib3==2.2.2\n + uvicorn==0.30.6\n + uvloop==0.20.0\n + vllm==0.5.4\n + vllm-flash-attn==2.6.1\n + watchfiles==0.23.0\n + websockets==13.0\n + xformers==0.0.27.post2\n + xxhash==3.5.0\n + yarl==1.9.4\n" status_code: 0
2024-08-22 12:26:45,264 - __main__ - INFO - Gathering system info
2024-08-22 12:26:49,096 - tools.ssh - INFO - Command: '\n    set -exo pipefail\n    \n    cd ~/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada\n    export PATH=$HOME/.cargo/bin:$PATH\n    source .venv/bin/activate;\n     python -m deterministic_ml._internal.sysinfo > ~/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/output/sysinfo.yaml' stdout: '' stderr: "+ cd /root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada\n+ export PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ source .venv/bin/activate\n++ '[' -n x ']'\n++ SCRIPT_PATH=.venv/bin/activate\n++ '[' .venv/bin/activate = bash ']'\n++ deactivate nondestructive\n++ unset -f pydoc\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ hash -r\n++ '[' -z '' ']'\n++ unset VIRTUAL_ENV\n++ unset VIRTUAL_ENV_PROMPT\n++ '[' '!' nondestructive = nondestructive ']'\n++ VIRTUAL_ENV=/root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/.venv\n++ '[' linux-gnu = cygwin ']'\n++ '[' linux-gnu = msys ']'\n++ export VIRTUAL_ENV\n++ _OLD_VIRTUAL_PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ PATH=/root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/.venv/bin:/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ '[' x2024-08-22_12-26-03_4x_rtx_4000ada '!=' x ']'\n++ VIRTUAL_ENV_PROMPT=2024-08-22_12-26-03_4x_rtx_4000ada\n++ export VIRTUAL_ENV_PROMPT\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ _OLD_VIRTUAL_PS1=\n++ PS1='(2024-08-22_12-26-03_4x_rtx_4000ada) '\n++ export PS1\n++ alias pydoc\n++ true\n++ hash -r\n+ python -m deterministic_ml._internal.sysinfo\n" status_code: 0
2024-08-22 12:26:49,110 - __main__ - INFO - Running experiment code on remote
2024-08-22 12:35:57,340 - tools.ssh - INFO - Command: '\n    set -exo pipefail\n    \n    cd ~/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada\n    export PATH=$HOME/.cargo/bin:$PATH\n    source .venv/bin/activate;\n     python -m vllm_llama_3_70b_instruct_awq ~/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/output | tee ~/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/output/stdout.txt' stdout: "gpu_count=4\nStarting model loading\nINFO 08-22 10:26:56 awq_marlin.py:89] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 08-22 10:26:56 config.py:729] Defaulting to use mp for distributed inference\nINFO 08-22 10:26:56 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='casperhansen/llama-3-70b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3-70b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=casperhansen/llama-3-70b-instruct-awq, use_v2_block_manager=False, enable_prefix_caching=False)\nWARNING 08-22 10:26:58 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 08-22 10:26:58 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n\x1b[1;36m(VllmWorkerProcess pid=789)\x1b[0;0m INFO 08-22 10:26:58 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n\x1b[1;36m(VllmWorkerProcess pid=787)\x1b[0;0m INFO 08-22 10:26:58 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m INFO 08-22 10:26:58 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m INFO 08-22 10:27:00 utils.py:841] Found nccl from library libnccl.so.2\nINFO 08-22 10:27:00 utils.py:841] Found nccl from library libnccl.so.2\n\x1b[1;36m(VllmWorkerProcess pid=787)\x1b[0;0m INFO 08-22 10:27:00 utils.py:841] Found nccl from library libnccl.so.2\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m INFO 08-22 10:27:00 pynccl.py:63] vLLM is using nccl==2.20.5\n\x1b[1;36m(VllmWorkerProcess pid=789)\x1b[0;0m INFO 08-22 10:27:00 utils.py:841] Found nccl from library libnccl.so.2\nINFO 08-22 10:27:00 pynccl.py:63] vLLM is using nccl==2.20.5\n\x1b[1;36m(VllmWorkerProcess pid=789)\x1b[0;0m INFO 08-22 10:27:00 pynccl.py:63] vLLM is using nccl==2.20.5\n\x1b[1;36m(VllmWorkerProcess pid=787)\x1b[0;0m INFO 08-22 10:27:00 pynccl.py:63] vLLM is using nccl==2.20.5\n\x1b[1;36m(VllmWorkerProcess pid=787)\x1b[0;0m WARNING 08-22 10:27:00 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m WARNING 08-22 10:27:00 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 08-22 10:27:00 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n\x1b[1;36m(VllmWorkerProcess pid=789)\x1b[0;0m WARNING 08-22 10:27:00 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 08-22 10:27:00 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7e9dd0cc7590>, local_subscribe_port=40373, remote_subscribe_port=None)\nINFO 08-22 10:27:00 model_runner.py:720] Starting to load model casperhansen/llama-3-70b-instruct-awq...\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m INFO 08-22 10:27:00 model_runner.py:720] Starting to load model casperhansen/llama-3-70b-instruct-awq...\n\x1b[1;36m(VllmWorkerProcess pid=787)\x1b[0;0m INFO 08-22 10:27:00 model_runner.py:720] Starting to load model casperhansen/llama-3-70b-instruct-awq...\n\x1b[1;36m(VllmWorkerProcess pid=789)\x1b[0;0m INFO 08-22 10:27:00 model_runner.py:720] Starting to load model casperhansen/llama-3-70b-instruct-awq...\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m INFO 08-22 10:27:00 weight_utils.py:225] Using model weights format ['*.safetensors']\n\x1b[1;36m(VllmWorkerProcess pid=789)\x1b[0;0m INFO 08-22 10:27:00 weight_utils.py:225] Using model weights format ['*.safetensors']\n\x1b[1;36m(VllmWorkerProcess pid=787)\x1b[0;0m INFO 08-22 10:27:00 weight_utils.py:225] Using model weights format ['*.safetensors']\nINFO 08-22 10:27:01 weight_utils.py:225] Using model weights format ['*.safetensors']\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m INFO 08-22 10:30:35 model_runner.py:732] Loading model weights took 9.2867 GB\n\x1b[1;36m(VllmWorkerProcess pid=789)\x1b[0;0m INFO 08-22 10:30:35 model_runner.py:732] Loading model weights took 9.2867 GB\n\x1b[1;36m(VllmWorkerProcess pid=787)\x1b[0;0m INFO 08-22 10:30:35 model_runner.py:732] Loading model weights took 9.2867 GB\nINFO 08-22 10:30:36 model_runner.py:732] Loading model weights took 9.2867 GB\nINFO 08-22 10:30:44 distributed_gpu_executor.py:56] # GPU blocks: 3207, # CPU blocks: 3276\nmodel loading took 233.71 seconds\nStarting 8 responses generation\n8 responses generation took 298.52 seconds\n{'Count to 1000, skip unpopular numbers': 'e01a486cc144586ae8b3b56ac3ea584290fbe07834a67a8dbc9ef98c66015d87d9abd0bcee16e90850ca183cdc948abcf208fc1d38a3ee2f8e4851cac05c10d9',\n 'Describe justice system in UK vs USA in 2000-5000 words': 'f12666eaf529cb993f9b5a24a9f3f9a336e0492c6fb45030acc46117776656ff5fff12fe03ba63ba431ffd32dfe68bc75a146059756f3925f0fbd1b39e01f1f8',\n 'Describe schooling system in UK vs USA in 2000-5000 words': '291023c3134a2fc4dc6f00507a428d9c7a6e166e1a0a7f73d74b0b4b60e460d6a50d143ea21b9cb9c26c10dd96002f208b0f8750dfc1b07cb5c742ff3c398fd2',\n 'Explain me some random problem for me in 2000-5000 words': '6c10b7cfd03339881798d66c02e1be1f99e5536746d82517435c3ab26bb5f6b377540fb2d374af62bacc1557de85f0d70b7f753ec074bde161d150c94382a833',\n 'Tell me entire history of USA': '1d193ab043b6dd23922e8258d6e134f390cebae90131340d47bf46510a2f34a4f93a5112b1e9160fe51219d2169576cda7948d605b4cb0d603d24388ee862687',\n 'Write a ballad. Pick a random theme.': '53aa9308f203c0f71abf485420b4a87411b63ea75535d2c708226963ddf29b926db30b7f21c690af5bb914ab6b4f659685d1bda1d14899813dffd2de5fcdef7f',\n 'Write an epic story about a dragon and a knight': 'e36cfba48cfa0862ad305c3f54543b0d7e9c44f89bdb6fb7d74168a8f1d3a5140b20644c62eda22795099daf3d5db93b8bd39fbb6394c5d6d5c41761cc253ce6',\n 'Write an essay about being a Senior developer.': '33deb94b55d7c18d7b3a2b564c0413a25a2eeacd152519a5884a3fa7c8f078a01edea1d7536f05e582647dfdbb218630579071030e5ea016e8a957ac5e4057d0'}\nERROR 08-22 10:35:51 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 788 died, exit code: -15\nINFO 08-22 10:35:51 multiproc_worker_utils.py:123] Killing local vLLM worker processes\n" stderr: "+ cd /root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada\n+ export PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ source .venv/bin/activate\n++ '[' -n x ']'\n++ SCRIPT_PATH=.venv/bin/activate\n++ '[' .venv/bin/activate = bash ']'\n++ deactivate nondestructive\n++ unset -f pydoc\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ hash -r\n++ '[' -z '' ']'\n++ unset VIRTUAL_ENV\n++ unset VIRTUAL_ENV_PROMPT\n++ '[' '!' nondestructive = nondestructive ']'\n++ VIRTUAL_ENV=/root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/.venv\n++ '[' linux-gnu = cygwin ']'\n++ '[' linux-gnu = msys ']'\n++ export VIRTUAL_ENV\n++ _OLD_VIRTUAL_PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ PATH=/root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/.venv/bin:/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ '[' x2024-08-22_12-26-03_4x_rtx_4000ada '!=' x ']'\n++ VIRTUAL_ENV_PROMPT=2024-08-22_12-26-03_4x_rtx_4000ada\n++ export VIRTUAL_ENV_PROMPT\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ _OLD_VIRTUAL_PS1=\n++ PS1='(2024-08-22_12-26-03_4x_rtx_4000ada) '\n++ export PS1\n++ alias pydoc\n++ true\n++ hash -r\n+ python -m vllm_llama_3_70b_instruct_awq /root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/output\n+ tee /root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/output/stdout.txt\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/5d/51/5d5111f0b358d39407f5182b8ea3ee71a6b1ed7942bd42d40a40c060adb2c2fb/dd7daa0a6f7e9a11ac7c28bd6dbbd6974b99efbe329afb15cc506fb705e6e407?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00009.safetensors%3B+filename%3D%22model-00002-of-00009.safetensors%22%3B&Expires=1724581621&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNDU4MTYyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzVkLzUxLzVkNTExMWYwYjM1OGQzOTQwN2Y1MTgyYjhlYTNlZTcxYTZiMWVkNzk0MmJkNDJkNDBhNDBjMDYwYWRiMmMyZmIvZGQ3ZGFhMGE2ZjdlOWExMWFjN2MyOGJkNmRiYmQ2OTc0Yjk5ZWZiZTMyOWFmYjE1Y2M1MDZmYjcwNWU2ZTQwNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=nlfFzTyogK4EsH6wbheWxniQwOIRKC6WNSOaFQQSQyXV0lvX2CpbwIv6JZwmqC8grxmPJHeKpXFUxg-5nW3sghqJGu756q2niYpV%7EC8HgwkUWFhI0uSV%7EdAYY4kQ%7E7c4b7dmY8hxqx3RsWZqyXNfr1R0l39Q39G0sTn6yiqLDq7f%7Ezpexl2xH1pzJ3EJFDRd1QBiLJVMd7Rh4yzHiBtmsNYuBgV5VCX%7Ezl-bc33zNonQfV%7E7k1VhTJ2sTsLGVIbW0CaboZZyqo8opAL1MMfYvI9hTMM10FQI-gMWRN9xcmWe7EWK%7EB2M-x6KkKK-5IimWoKd5fsYvF8jZmie6epALA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m Trying to resume download...\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/5d/51/5d5111f0b358d39407f5182b8ea3ee71a6b1ed7942bd42d40a40c060adb2c2fb/bf8de6ef3f4e527721c9f03c5bec9dd6219b58623d11feef6c213a6fee79d759?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00003-of-00009.safetensors%3B+filename%3D%22model-00003-of-00009.safetensors%22%3B&Expires=1724581621&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNDU4MTYyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzVkLzUxLzVkNTExMWYwYjM1OGQzOTQwN2Y1MTgyYjhlYTNlZTcxYTZiMWVkNzk0MmJkNDJkNDBhNDBjMDYwYWRiMmMyZmIvYmY4ZGU2ZWYzZjRlNTI3NzIxYzlmMDNjNWJlYzlkZDYyMTliNTg2MjNkMTFmZWVmNmMyMTNhNmZlZTc5ZDc1OT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=McjODK0UDiZHwiqwC4q6oncp3zJEfm4K9rLznT7rZW2IYk2Y1Rv7vACZbm9xpH5kOK%7Er1Qh3bTVmMDB29rKPSmswjBNQYVW-IDpCSS3hWNvzHBE7HvdQVB9%7Ej5vm%7EkWKgYJUUDbeSePmJ-vt%7EfHxBmHpm5UfsfZJZtNNl62s1ss1XL5kNPmwIaeZHWpmuVK7rXaeJQZMlGYSvnnFMs0eDviVbr0-6pGjHZgiC4HoiF7290GO-TgPISRUsVPK-iVhjOGFafUDzZAeJAqdS%7ENJiX47VR5012YvTGdTX0mWcZUSMEt-vaNsImbSro642d%7EWVxTCsyX%7E2wRLWlUtPOOkQA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n\x1b[1;36m(VllmWorkerProcess pid=788)\x1b[0;0m Trying to resume download...\n\rLoading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n\rLoading safetensors checkpoint shards:  11% Completed | 1/9 [00:02<00:18,  2.25s/it]\n\rLoading safetensors checkpoint shards:  22% Completed | 2/9 [00:05<00:19,  2.82s/it]\n\rLoading safetensors checkpoint shards:  33% Completed | 3/9 [00:08<00:18,  3.02s/it]\n\rLoading safetensors checkpoint shards:  44% Completed | 4/9 [00:11<00:15,  3.11s/it]\n\rLoading safetensors checkpoint shards:  56% Completed | 5/9 [00:15<00:12,  3.23s/it]\n\rLoading safetensors checkpoint shards:  67% Completed | 6/9 [00:18<00:09,  3.22s/it]\n\rLoading safetensors checkpoint shards:  78% Completed | 7/9 [00:20<00:05,  2.70s/it]\n\rLoading safetensors checkpoint shards:  89% Completed | 8/9 [00:23<00:02,  2.89s/it]\n\rLoading safetensors checkpoint shards: 100% Completed | 9/9 [00:26<00:00,  3.01s/it]\n\rLoading safetensors checkpoint shards: 100% Completed | 9/9 [00:26<00:00,  2.98s/it]\n\n/root/experiments/vllm_llama_3_70b_instruct_awq/2024-08-22_12-26-03_4x_rtx_4000ada/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:287: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n  probs_sum = probs_sort.cumsum(dim=-1)\n\rProcessed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\rProcessed prompts:  12%|█▎        | 1/8 [03:33<24:53, 213.34s/it, est. speed input: 0.18 toks/s, output: 13.73 toks/s]\rProcessed prompts:  38%|███▊      | 3/8 [04:58<07:14, 86.86s/it, est. speed input: 0.35 toks/s, output: 33.35 toks/s] \rProcessed prompts: 100%|██████████| 8/8 [04:58<00:00, 37.31s/it, est. speed input: 0.94 toks/s, output: 101.95 toks/s]\n/root/.local/share/uv/python/cpython-3.11.9-linux-x86_64-gnu/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n" status_code: 0
2024-08-22 12:35:57,409 - __main__ - INFO - Syncing output back to local
2024-08-22 12:35:57,834 - __main__ - INFO - Done
