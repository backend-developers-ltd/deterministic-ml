2024-08-28 10:03:04,205 - __main__ - INFO - Starting experiment vllm_phi35 with comment: 1x RTX 3090
2024-08-28 10:03:04,208 - __main__ - INFO - Local log file: /home/rooter/dev/bac/deterministic-ml/tests/integration/results/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/run.local.log
2024-08-28 10:03:04,345 - paramiko.transport - INFO - Connected (version 2.0, client OpenSSH_8.9p1)
2024-08-28 10:03:04,631 - paramiko.transport - INFO - Auth banner: b'Welcome to vast.ai. If authentication fails, try again after a few seconds, and double check your ssh key.\nHave fun!\n'
2024-08-28 10:03:04,637 - paramiko.transport - INFO - Authentication (publickey) successful!
2024-08-28 10:03:04,640 - __main__ - INFO - Syncing files to remote
2024-08-28 10:03:04,873 - tools.ssh - INFO - Command: 'mkdir -p ~/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/output' stdout: '' stderr: '' status_code: 0
2024-08-28 10:03:07,782 - __main__ - INFO - Setting up remote environment
2024-08-28 10:03:10,855 - tools.ssh - INFO - Command: '\n    set -exo pipefail\n    \n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=$HOME/.cargo/bin:$PATH\n    \n    cd ~/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090\n    uv venv -p python3.11 --python-preference managed\n    source .venv/bin/activate \n    uv pip install       ./deterministic_ml*.whl       pyyaml       -r vllm_phi35/requirements.txt\n    ' stdout: "installing to /root/.cargo/bin\n  uv\n  uvx\neverything's installed!\n" stderr: "+ sh\n+ curl -LsSf https://astral.sh/uv/install.sh\ndownloading uv 0.3.5 x86_64-unknown-linux-gnu\n+ export PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ cd /root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090\n+ uv venv -p python3.11 --python-preference managed\nUsing Python 3.11.9\nCreating virtualenv at: .venv\nActivate with: source .venv/bin/activate\n+ source .venv/bin/activate\n++ '[' -n x ']'\n++ SCRIPT_PATH=.venv/bin/activate\n++ '[' .venv/bin/activate = bash ']'\n++ deactivate nondestructive\n++ unset -f pydoc\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ hash -r\n++ '[' -z '' ']'\n++ unset VIRTUAL_ENV\n++ unset VIRTUAL_ENV_PROMPT\n++ '[' '!' nondestructive = nondestructive ']'\n++ VIRTUAL_ENV=/root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/.venv\n++ '[' linux-gnu = cygwin ']'\n++ '[' linux-gnu = msys ']'\n++ export VIRTUAL_ENV\n++ _OLD_VIRTUAL_PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ PATH=/root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/.venv/bin:/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ '[' x2024-08-28_10-03-04_1x_rtx_3090 '!=' x ']'\n++ VIRTUAL_ENV_PROMPT=2024-08-28_10-03-04_1x_rtx_3090\n++ export VIRTUAL_ENV_PROMPT\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ _OLD_VIRTUAL_PS1=\n++ PS1='(2024-08-28_10-03-04_1x_rtx_3090) '\n++ export PS1\n++ alias pydoc\n++ true\n++ hash -r\n+ uv pip install ./deterministic_ml-0.1.dev12+g033d6ad.d20240828-py3-none-any.whl pyyaml -r vllm_phi35/requirements.txt\nResolved 124 packages in 57ms\nPrepared 1 package in 1ms\nInstalled 124 packages in 260ms\n + aiohappyeyeballs==2.4.0\n + aiohttp==3.10.5\n + aiosignal==1.3.1\n + annotated-types==0.7.0\n + anyio==4.4.0\n + attrs==24.2.0\n + audioread==3.0.1\n + certifi==2024.7.4\n + cffi==1.17.0\n + charset-normalizer==3.3.2\n + click==8.1.7\n + cloudpickle==3.0.0\n + datasets==2.21.0\n + decorator==5.1.1\n + deterministic-ml==0.1.dev12+g033d6ad.d20240828 (from file:///root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/deterministic_ml-0.1.dev12+g033d6ad.d20240828-py3-none-any.whl)\n + dill==0.3.8\n + diskcache==5.6.3\n + distro==1.9.0\n + fastapi==0.112.2\n + filelock==3.15.4\n + frozenlist==1.4.1\n + fsspec==2024.6.1\n + gguf==0.9.1\n + h11==0.14.0\n + httpcore==1.0.5\n + httptools==0.6.1\n + httpx==0.27.2\n + huggingface-hub==0.24.6\n + idna==3.8\n + importlib-metadata==8.4.0\n + interegular==0.3.3\n + jinja2==3.1.4\n + jiter==0.5.0\n + joblib==1.4.2\n + jsonschema==4.23.0\n + jsonschema-specifications==2023.12.1\n + lark==1.2.2\n + lazy-loader==0.4\n + librosa==0.10.2.post1\n + llvmlite==0.43.0\n + lm-format-enforcer==0.10.6\n + markupsafe==2.1.5\n + mpmath==1.3.0\n + msgpack==1.0.8\n + msgspec==0.18.6\n + multidict==6.0.5\n + multiprocess==0.70.16\n + nest-asyncio==1.6.0\n + networkx==3.3\n + numba==0.60.0\n + numpy==1.26.4\n + nvidia-cublas-cu12==12.1.3.1\n + nvidia-cuda-cupti-cu12==12.1.105\n + nvidia-cuda-nvrtc-cu12==12.1.105\n + nvidia-cuda-runtime-cu12==12.1.105\n + nvidia-cudnn-cu12==9.1.0.70\n + nvidia-cufft-cu12==11.0.2.54\n + nvidia-curand-cu12==10.3.2.106\n + nvidia-cusolver-cu12==11.4.5.107\n + nvidia-cusparse-cu12==12.1.0.106\n + nvidia-ml-py==12.560.30\n + nvidia-nccl-cu12==2.20.5\n + nvidia-nvjitlink-cu12==12.6.20\n + nvidia-nvtx-cu12==12.1.105\n + openai==1.42.0\n + outlines==0.0.46\n + packaging==24.1\n + pandas==2.2.2\n + pillow==10.4.0\n + platformdirs==4.2.2\n + pooch==1.8.2\n + prometheus-client==0.20.0\n + prometheus-fastapi-instrumentator==7.0.0\n + protobuf==5.27.4\n + psutil==6.0.0\n + py-cpuinfo==9.0.0\n + pyairports==2.1.1\n + pyarrow==17.0.0\n + pycountry==24.6.1\n + pycparser==2.22\n + pydantic==2.8.2\n + pydantic-core==2.20.1\n + python-dateutil==2.9.0.post0\n + python-dotenv==1.0.1\n + pytz==2024.1\n + pyyaml==6.0.2\n + pyzmq==26.2.0\n + ray==2.35.0\n + referencing==0.35.1\n + regex==2024.7.24\n + requests==2.32.3\n + rpds-py==0.20.0\n + safetensors==0.4.4\n + scikit-learn==1.5.1\n + scipy==1.14.1\n + sentencepiece==0.2.0\n + setuptools==74.0.0\n + six==1.16.0\n + sniffio==1.3.1\n + soundfile==0.12.1\n + soxr==0.5.0\n + starlette==0.38.2\n + sympy==1.13.2\n + threadpoolctl==3.5.0\n + tiktoken==0.7.0\n + tokenizers==0.19.1\n + torch==2.4.0\n + torchvision==0.19.0\n + tqdm==4.66.5\n + transformers==4.44.2\n + triton==3.0.0\n + typing-extensions==4.12.2\n + tzdata==2024.1\n + urllib3==2.2.2\n + uvicorn==0.30.6\n + uvloop==0.20.0\n + vllm==0.5.5\n + vllm-flash-attn==2.6.1\n + watchfiles==0.23.0\n + websockets==13.0\n + xformers==0.0.27.post2\n + xxhash==3.5.0\n + yarl==1.9.4\n + zipp==3.20.1\n" status_code: 0
2024-08-28 10:03:10,868 - __main__ - INFO - Gathering system info
2024-08-28 10:03:14,017 - tools.ssh - INFO - Command: '\n    set -exo pipefail\n    \n    cd ~/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090\n    export PATH=$HOME/.cargo/bin:$PATH\n    source .venv/bin/activate;\n     python -m deterministic_ml._internal.sysinfo > ~/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/output/sysinfo.yaml' stdout: '' stderr: "+ cd /root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090\n+ export PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ source .venv/bin/activate\n++ '[' -n x ']'\n++ SCRIPT_PATH=.venv/bin/activate\n++ '[' .venv/bin/activate = bash ']'\n++ deactivate nondestructive\n++ unset -f pydoc\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ hash -r\n++ '[' -z '' ']'\n++ unset VIRTUAL_ENV\n++ unset VIRTUAL_ENV_PROMPT\n++ '[' '!' nondestructive = nondestructive ']'\n++ VIRTUAL_ENV=/root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/.venv\n++ '[' linux-gnu = cygwin ']'\n++ '[' linux-gnu = msys ']'\n++ export VIRTUAL_ENV\n++ _OLD_VIRTUAL_PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ PATH=/root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/.venv/bin:/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ '[' x2024-08-28_10-03-04_1x_rtx_3090 '!=' x ']'\n++ VIRTUAL_ENV_PROMPT=2024-08-28_10-03-04_1x_rtx_3090\n++ export VIRTUAL_ENV_PROMPT\n++ '[' -z '' ']'\n++ '[' -z '' ']'\n++ _OLD_VIRTUAL_PS1=\n++ PS1='(2024-08-28_10-03-04_1x_rtx_3090) '\n++ export PS1\n++ alias pydoc\n++ true\n++ hash -r\n+ python -m deterministic_ml._internal.sysinfo\n" status_code: 0
2024-08-28 10:03:14,020 - __main__ - INFO - Running experiment code on remote
2024-08-28 10:05:32,402 - tools.ssh - INFO - Command: '\n    set -exo pipefail\n    \n    cd ~/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090\n    export PATH=$HOME/.cargo/bin:$PATH\n    source .venv/bin/activate;\n     python -m vllm_phi35 ~/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/output | tee ~/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/output/stdout.txt' stdout: "Starting model loading\nINFO 08-28 08:03:24 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='microsoft/Phi-3.5-mini-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=cd6881a82d62252f5a84593c61acf290f15d89e3, rope_scaling=None, rope_theta=None, tokenizer_revision=cd6881a82d62252f5a84593c61acf290f15d89e3, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/Phi-3.5-mini-instruct, use_v2_block_manager=False, enable_prefix_caching=False)\nINFO 08-28 08:03:24 selector.py:236] Cannot use FlashAttention-2 backend due to sliding window.\nINFO 08-28 08:03:24 selector.py:116] Using XFormers backend.\nINFO 08-28 08:03:25 model_runner.py:879] Starting to load model microsoft/Phi-3.5-mini-instruct...\nINFO 08-28 08:03:25 selector.py:236] Cannot use FlashAttention-2 backend due to sliding window.\nINFO 08-28 08:03:25 selector.py:116] Using XFormers backend.\nINFO 08-28 08:03:26 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 08-28 08:03:32 model_runner.py:890] Loading model weights took 7.1659 GB\nINFO 08-28 08:03:33 gpu_executor.py:121] # GPU blocks: 2217, # CPU blocks: 682\nmodel loading took 10.75 seconds\nStarting 8 responses generation\n8 responses generation took 116.15 seconds\n{'Count to 1000, skip unpopular numbers': 'fe0119a32e5cb10c8ef7b32824e01f72ed7dacb9619e4e54df43b05a8ce6d978586fcc462b8ef7734ca6331cb5e383f1621f246647a16b6420e2d48f5f63c15c',\n 'Describe justice system in UK vs USA in 2000-5000 words': 'f18f930c23ed062a36f753c4f45da2dee51c9e9c24f2314424e002a8b67cb5f6e9a2525ddcabe7e314c593ce6c86f96f05325f6d6bc98a859dfe5ca5ce8e9d98',\n 'Describe schooling system in UK vs USA in 2000-5000 words': '1902d972aedee49f8aca9d0dd03eff7997d143c84e274296db434c3d2e71b1171233d8b824f989861f51d120c8e70f792390136e311d72152bc52a3212cd29ad',\n 'Explain me some random problem for me in 2000-5000 words': '2953a6cc64465b5c62dcb8257d3292304ea3b9a43a2ce01c82bcd8a59f94b2cc01e1217d7769ceb248bd64c4f11e3f2a60ff79e986e3fb64443aae0f902ac58e',\n 'Tell me entire history of USA': '678c0e2d3fe6dab2eb6c3eba428f98c190b784fd35f16183025a341e4a08c98f39e6aa43d3ed2bf8dc96464d02ee20545f4db73bcb790c57d7e4db781499dfd5',\n 'Write a ballad. Pick a random theme.': 'a2ca9c83288ba84b99d33a391b7b83747a263486c23958d0c30eb35299cd33236371dc78ed4bd9d0e453185878fdc0317117d2b23507bc2bb341279ffdd5d9c4',\n 'Write an epic story about a dragon and a knight': 'b7e133385c527856f91c6d26e16fa5590074d349729aa45b0cc80f2e2df993cb4e7e4a30d2bae4265d310153a71e07086a0121083c895a8cad3406dd2922d945',\n 'Write an essay about being a Senior developer.': '9119725699bc047553df0eb90d3ba431d9e04c52b201cb4eaed599df120f66c08f06449454623447ec7f74b651bca652106bcba9950f881fae8ae27e0f29ffa4'}\n" stderr: '+ cd /root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090\n+ export PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n+ source .venv/bin/activate\n++ \'[\' -n x \']\'\n++ SCRIPT_PATH=.venv/bin/activate\n++ \'[\' .venv/bin/activate = bash \']\'\n++ deactivate nondestructive\n++ unset -f pydoc\n++ \'[\' -z \'\' \']\'\n++ \'[\' -z \'\' \']\'\n++ hash -r\n++ \'[\' -z \'\' \']\'\n++ unset VIRTUAL_ENV\n++ unset VIRTUAL_ENV_PROMPT\n++ \'[\' \'!\' nondestructive = nondestructive \']\'\n++ VIRTUAL_ENV=/root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/.venv\n++ \'[\' linux-gnu = cygwin \']\'\n++ \'[\' linux-gnu = msys \']\'\n++ export VIRTUAL_ENV\n++ _OLD_VIRTUAL_PATH=/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ PATH=/root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/.venv/bin:/root/.cargo/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n++ export PATH\n++ \'[\' x2024-08-28_10-03-04_1x_rtx_3090 \'!=\' x \']\'\n++ VIRTUAL_ENV_PROMPT=2024-08-28_10-03-04_1x_rtx_3090\n++ export VIRTUAL_ENV_PROMPT\n++ \'[\' -z \'\' \']\'\n++ \'[\' -z \'\' \']\'\n++ _OLD_VIRTUAL_PS1=\n++ PS1=\'(2024-08-28_10-03-04_1x_rtx_3090) \'\n++ export PS1\n++ alias pydoc\n++ true\n++ hash -r\n+ python -m vllm_phi35 /root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/output\n+ tee /root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/output/stdout.txt\n/root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/.venv/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract("xformers_flash::flash_fwd")\n/root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/.venv/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract("xformers_flash::flash_bwd")\n\rLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\rLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.71s/it]\n\rLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.75s/it]\n\rLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.59s/it]\n\n/root/experiments/vllm_phi35/2024-08-28_10-03-04_1x_rtx_3090/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:301: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set \'torch.use_deterministic_algorithms(True, warn_only=True)\'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n  probs_sum = probs_sort.cumsum(dim=-1)\n\rProcessed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\rProcessed prompts:  12%|█▎        | 1/8 [00:20<02:24, 20.66s/it, est. speed input: 4.98 toks/s, output: 34.60 toks/s]\rProcessed prompts:  25%|██▌       | 2/8 [00:23<00:59, 10.00s/it, est. speed input: 9.01 toks/s, output: 65.36 toks/s]\rProcessed prompts:  38%|███▊      | 3/8 [00:26<00:34,  6.98s/it, est. speed input: 11.74 toks/s, output: 91.56 toks/s]\rProcessed prompts:  50%|█████     | 4/8 [00:27<00:19,  4.77s/it, est. speed input: 15.19 toks/s, output: 121.56 toks/s]\rProcessed prompts:  62%|██████▎   | 5/8 [00:32<00:13,  4.58s/it, est. speed input: 16.74 toks/s, output: 140.15 toks/s]\rProcessed prompts:  75%|███████▌  | 6/8 [00:52<00:19,  9.90s/it, est. speed input: 12.43 toks/s, output: 120.73 toks/s]\rProcessed prompts:  88%|████████▊ | 7/8 [01:54<00:27, 27.01s/it, est. speed input: 6.61 toks/s, output: 90.02 toks/s]  \rProcessed prompts: 100%|██████████| 8/8 [01:56<00:00, 18.88s/it, est. speed input: 7.38 toks/s, output: 123.67 toks/s]\rProcessed prompts: 100%|██████████| 8/8 [01:56<00:00, 14.52s/it, est. speed input: 7.38 toks/s, output: 123.67 toks/s]\n' status_code: 0
2024-08-28 10:05:32,417 - __main__ - INFO - Syncing output back to local
2024-08-28 10:05:33,108 - __main__ - INFO - Done
